{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1552c519-f54f-4796-b15a-ed89eb932c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: A100-PCIE-40GB, pci bus id: 0000:c3:00.0, compute capability: 8.0\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Multilayer Perceptron for drug response problem\"\"\"\n",
    "\n",
    "from __future__ import division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, BatchNormalization, Dense, Dropout, LocallyConnected1D, Conv1D, MaxPooling1D, Flatten, Conv2D, LocallyConnected2D\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ProgbarLogger\n",
    "\n",
    "# For non-interactive plotting\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import p1b3 as benchmark\n",
    "import candle\n",
    "\n",
    "sys.argv = [''] # for Jupyter nbs\n",
    "\n",
    "#cfg = K.tf.ConfigProto(gpu_options={'allow_growth': True})\n",
    "#K.set_session(K.tf.Session(config=cfg))\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab46ec3c-f742-4fc3-a7d8-d84f8f960a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(default_model = 'p1b3_default_model.txt'):\n",
    "    \n",
    "    # Build benchmark object\n",
    "    p1b3Bmk = benchmark.BenchmarkP1B3(benchmark.file_path, default_model, 'keras',\n",
    "    prog='p1b3_baseline', desc='Multi-task (DNN) for data extraction from clinical reports - Pilot 3 Benchmark 1')\n",
    "    \n",
    "    \n",
    "    # Initialize parameters\n",
    "    gParameters = candle.finalize_parameters(p1b3Bmk)\n",
    "    #benchmark.logger.info('Params: {}'.format(gParameters))\n",
    "\n",
    "    return gParameters\n",
    "\n",
    "def str2lst(string_val):\n",
    "    result = [int(x) for x in string_val.split(' ')]\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_keras_metric(y_true, y_pred, metric):\n",
    "    objective_function = metrics.get(metric)\n",
    "    objective = objective_function(y_true, y_pred)\n",
    "    return K.eval(objective)\n",
    "\n",
    "\n",
    "def evaluate_model(model, generator, steps, metric, category_cutoffs=[0.]):\n",
    "    y_true, y_pred = None, None\n",
    "    count = 0\n",
    "    while count < steps:\n",
    "        x_batch, y_batch = next(generator)\n",
    "        y_batch_pred = model.predict_on_batch(x_batch)\n",
    "        y_batch_pred = y_batch_pred.ravel()\n",
    "        y_true = np.concatenate((y_true, y_batch)) if y_true is not None else y_batch\n",
    "        y_pred = np.concatenate((y_pred, y_batch_pred)) if y_pred is not None else y_batch_pred\n",
    "        count += 1\n",
    "\n",
    "    loss = evaluate_keras_metric(y_true.astype(np.float32), y_pred.astype(np.float32), metric)\n",
    "\n",
    "    y_true_class = np.digitize(y_true, category_cutoffs)\n",
    "    y_pred_class = np.digitize(y_pred, category_cutoffs)\n",
    "\n",
    "    # theano does not like integer input\n",
    "    acc = evaluate_keras_metric(y_true_class.astype(np.float32), y_pred_class.astype(np.float32), 'binary_accuracy')  # works for multiclass labels as well\n",
    "\n",
    "    return loss, acc, y_true, y_pred, y_true_class, y_pred_class\n",
    "\n",
    "\n",
    "def plot_error(y_true, y_pred, batch, file_ext, file_pre='output_dir', subsample=1000):\n",
    "    if batch % 10:\n",
    "        return\n",
    "\n",
    "    total = len(y_true)\n",
    "    if subsample and subsample < total:\n",
    "        usecols = np.random.choice(total, size=subsample, replace=False)\n",
    "        y_true = y_true[usecols]\n",
    "        y_pred = y_pred[usecols]\n",
    "\n",
    "    y_true = y_true * 100\n",
    "    y_pred = y_pred * 100\n",
    "    diffs = y_pred - y_true\n",
    "\n",
    "    bins = np.linspace(-200, 200, 100)\n",
    "    if batch == 0:\n",
    "        y_shuf = np.random.permutation(y_true)\n",
    "        plt.hist(y_shuf - y_true, bins, alpha=0.5, label='Random')\n",
    "\n",
    "    #plt.hist(diffs, bins, alpha=0.35-batch/100., label='Epoch {}'.format(batch+1))\n",
    "    plt.hist(diffs, bins, alpha=0.3, label='Epoch {}'.format(batch+1))\n",
    "    plt.title(\"Histogram of errors in percentage growth\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(file_pre+'.histogram'+file_ext+'.b'+str(batch)+'.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot measured vs. predicted values\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.grid('on')\n",
    "    ax.scatter(y_true, y_pred, color='red', s=10)\n",
    "    ax.plot([y_true.min(), y_true.max()],\n",
    "            [y_true.min(), y_true.max()], 'k--', lw=4)\n",
    "    ax.set_xlabel('Measured')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    plt.savefig(file_pre+'.diff'+file_ext+'.b'+str(batch)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410c4124-7041-4a6e-a537-febe4f62b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLossHistory(Callback):\n",
    "    def __init__(self, progbar, val_gen, test_gen, val_steps, test_steps, metric, category_cutoffs=[0.], ext='', pre='save'):\n",
    "        super(MyLossHistory, self).__init__()\n",
    "        self.progbar = progbar\n",
    "        self.val_gen = val_gen\n",
    "        self.test_gen = test_gen\n",
    "        self.val_steps = val_steps\n",
    "        self.test_steps = test_steps\n",
    "        self.metric = metric\n",
    "        self.category_cutoffs = category_cutoffs\n",
    "        self.pre = pre\n",
    "        self.ext = ext\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.best_val_loss = np.Inf\n",
    "        self.best_val_acc = -np.Inf\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        val_loss, val_acc, y_true, y_pred, y_true_class, y_pred_class = evaluate_model(self.model, self.val_gen, self.val_steps, self.metric, self.category_cutoffs)\n",
    "        test_loss, test_acc, _, _, _, _ = evaluate_model(self.model, self.test_gen, self.test_steps, self.metric, self.category_cutoffs)\n",
    "        self.progbar.append_extra_log_values([('val_acc', val_acc), ('test_loss', test_loss), ('test_acc', test_acc)])\n",
    "        if float(logs.get('val_loss', 0)) < self.best_val_loss:\n",
    "            plot_error(y_true, y_pred, batch, self.ext, self.pre)\n",
    "        self.best_val_loss = min(float(logs.get('val_loss', 0)), self.best_val_loss)\n",
    "        self.best_val_acc = max(float(logs.get('val_acc', 0)), self.best_val_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8078b4c3-200a-4f8c-abe7-1afba4dce566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyProgbarLogger(ProgbarLogger):\n",
    "    def __init__(self, samples):\n",
    "        super(MyProgbarLogger, self).__init__(count_mode='samples')\n",
    "        self.samples = samples\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        super(MyProgbarLogger, self).on_train_begin(logs)\n",
    "        self.verbose = 1\n",
    "        self.extra_log_values = []\n",
    "        self.params['samples'] = self.samples\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if self.seen < self.target:\n",
    "            self.log_values = []\n",
    "            self.extra_log_values = []\n",
    "\n",
    "    def append_extra_log_values(self, tuples):\n",
    "        for k, v in tuples:\n",
    "            self.extra_log_values.append((k, v))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        epoch_log = 'Epoch {}/{}'.format(epoch + 1, self.epochs)\n",
    "        for k in self.params['metrics']:\n",
    "            if k in logs:\n",
    "                self.log_values.append((k, logs[k]))\n",
    "                epoch_log += ' - {}: {:.4f}'.format(k, logs[k])\n",
    "        for k, v in self.extra_log_values:\n",
    "            self.log_values.append((k, v))\n",
    "            epoch_log += ' - {}: {:.4f}'.format(k, float(v))\n",
    "        if self.verbose:\n",
    "            self.progbar.update(self.seen, self.log_values)\n",
    "        benchmark.logger.debug(epoch_log)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31320c48-f7c5-439e-aed3-b33e0f2e5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_conv_layer(model, layer_params, input_dim=None, locally_connected=False):\n",
    "    if len(layer_params) == 3: # 1D convolution\n",
    "        filters = layer_params[0]\n",
    "        filter_len = layer_params[1]\n",
    "        stride = layer_params[2]\n",
    "        if locally_connected:\n",
    "            if input_dim:\n",
    "                model.add(LocallyConnected1D(filters, filter_len, strides=stride, input_shape=(input_dim, 1)))\n",
    "            else:\n",
    "                model.add(LocallyConnected1D(filters, filter_len, strides=stride))\n",
    "        else:\n",
    "            if input_dim:\n",
    "                model.add(Conv1D(filters, filter_len, strides=stride, input_shape=(input_dim, 1)))\n",
    "            else:\n",
    "                model.add(Conv1D(filters, filter_len, strides=stride))\n",
    "    elif len(layer_params) == 5: # 2D convolution\n",
    "        filters = layer_params[0]\n",
    "        filter_len = (layer_params[1], layer_params[2])\n",
    "        stride = (layer_params[3], layer_params[4])\n",
    "        if locally_connected:\n",
    "            if input_dim:\n",
    "                model.add(LocallyConnected2D(filters, filter_len, strides=stride, input_shape=(input_dim, 1)))\n",
    "            else:\n",
    "                model.add(LocallyConnected2D(filters, filter_len, strides=stride))\n",
    "        else:\n",
    "            if input_dim:\n",
    "                model.add(Conv2D(filters, filter_len, strides=stride, input_shape=(input_dim, 1)))\n",
    "            else:\n",
    "                model.add(Conv2D(filters, filter_len, strides=stride))\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17f5c065-be38-4997-ae86-9a57b71e1737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:\n",
      "{'activation': 'relu',\n",
      " 'batch_normalization': False,\n",
      " 'batch_size': 100,\n",
      " 'category_cutoffs': [0.0],\n",
      " 'cell_features': ['expression'],\n",
      " 'cell_noise_sigma': 0.0,\n",
      " 'data_type': <class 'numpy.float32'>,\n",
      " 'dense': [1000, 500, 100, 50],\n",
      " 'dropout': 0.1,\n",
      " 'drug_features': ['descriptors'],\n",
      " 'epochs': 1,\n",
      " 'experiment_id': 'EXP000',\n",
      " 'feature_subsample': 0,\n",
      " 'initialization': 'normal',\n",
      " 'learning_rate': 0.001,\n",
      " 'logfile': None,\n",
      " 'loss': 'mse',\n",
      " 'max_logconc': -4.0,\n",
      " 'min_logconc': -5.0,\n",
      " 'optimizer': 'sgd',\n",
      " 'output_dir': '/global/u2/v/vineethg/NCI-DOE-Collab-Pilot1-Single-Drug-Response-Predictor/Pilot1/P1B3/save/EXP000/RUN000',\n",
      " 'profiling': False,\n",
      " 'rng_seed': 2017,\n",
      " 'run_id': 'RUN000',\n",
      " 'scaling': 'std',\n",
      " 'scramble': False,\n",
      " 'shuffle': False,\n",
      " 'subsample': 'naive_balancing',\n",
      " 'test_cell_split': 0.15,\n",
      " 'timeout': -1,\n",
      " 'train_bool': True,\n",
      " 'val_split': 0.1,\n",
      " 'verbose': None,\n",
      " 'workers': 1}\n"
     ]
    }
   ],
   "source": [
    "gParameters = initialize_parameters()\n",
    "benchmark.check_params(gParameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e8fbebf-a1ad-497d-8772-9cc707a23311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 500, 100, 50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded 2328562 unique (D, CL) response sets.\n",
      "/global/u2/v/vineethg/NCI-DOE-Collab-Pilot1-Single-Drug-Response-Predictor/Pilot1/P1B3/p1b3.py:488: ParserWarning: Both a converter and dtype were specified for column NAME - only the converter will be used\n",
      "  converters ={'NAME' : str})\n",
      "Distribution of dose response:\n",
      "             GROWTH\n",
      "count  1.014891e+06\n",
      "mean  -1.296296e+00\n",
      "std    6.216944e+01\n",
      "min   -1.000000e+02\n",
      "25%   -5.600000e+01\n",
      "50%    0.000000e+00\n",
      "75%    4.600000e+01\n",
      "max    2.700000e+02\n",
      "Category cutoffs: [0.0]\n",
      "Dose response bin counts:\n",
      "  Class 0:  502018 (0.4947) - between -1.00 and +0.00\n",
      "  Class 1:  512873 (0.5053) - between +0.00 and +2.70\n",
      "  Total:   1014891\n",
      "Rows in train: 900136, val: 100015, test: 14740\n",
      "Input features shapes:\n",
      "  drug_concentration: (1,)\n",
      "  cell_expression: (25722,)\n",
      "  drug_descriptors: (3809,)\n",
      "Total input dimensions: 29532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /global/homes/v/vineethg/.conda/envs/Combo/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "Loaded model weights from disk\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              40038000  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 40,593,701\n",
      "Trainable params: 40,593,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "if 'dense' in gParameters:\n",
    "    dval = gParameters['dense']\n",
    "    if type(dval) != list:\n",
    "        res = list(dval)\n",
    "    #try:\n",
    "        #is_str = isinstance(dval, basestring)\n",
    "    #except NameError:\n",
    "        #is_str = isinstance(dval, str)\n",
    "    #if is_str:\n",
    "        #res = str2lst(dval)\n",
    "        gParameters['dense'] = res\n",
    "    print(gParameters['dense'])\n",
    "\n",
    "if 'conv' in gParameters:\n",
    "    flat = gParameters['conv']\n",
    "    gParameters['conv'] = [flat[i:i+3] for i in range(0, len(flat), 3)]\n",
    "    #conv_list = p1_common.parse_conv_list(gParameters['conv'])\n",
    "    #cval = gParameters['conv']\n",
    "    #try:\n",
    "    #    is_str = isinstance(cval, basestring)\n",
    "    #except NameError:\n",
    "    #    is_str = isinstance(cval, str)\n",
    "    #if is_str:\n",
    "    #    res = str2lst(cval)\n",
    "    #    gParameters['conv'] = res\n",
    "    print('Conv input', gParameters['conv'])\n",
    "# print('Params:', gParameters)\n",
    "# Construct extension to save model\n",
    "ext = benchmark.extension_from_parameters(gParameters, '.keras')\n",
    "logfile = gParameters['logfile'] if gParameters['logfile'] else gParameters['output_dir']+ext+'.log'\n",
    "\n",
    "fh = logging.FileHandler(logfile)\n",
    "fh.setFormatter(logging.Formatter(\"[%(asctime)s %(process)d] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"))\n",
    "fh.setLevel(logging.DEBUG)\n",
    "\n",
    "sh = logging.StreamHandler()\n",
    "sh.setFormatter(logging.Formatter(''))\n",
    "sh.setLevel(logging.DEBUG if gParameters['verbose'] else logging.INFO)\n",
    "\n",
    "benchmark.logger.setLevel(logging.DEBUG)\n",
    "benchmark.logger.addHandler(fh)\n",
    "benchmark.logger.addHandler(sh)\n",
    "# benchmark.logger.info('Params: {}'.format(gParameters))\n",
    "\n",
    "# Get default parameters for initialization and optimizer functions\n",
    "kerasDefaults = candle.keras_default_config()\n",
    "seed = gParameters['rng_seed']\n",
    "\n",
    "# Build dataset loader object\n",
    "loader = benchmark.DataLoader(seed=seed, dtype=gParameters['data_type'],\n",
    "                         val_split=gParameters['val_split'],\n",
    "                         test_cell_split=gParameters['test_cell_split'],\n",
    "                         cell_features=gParameters['cell_features'],\n",
    "                         drug_features=gParameters['drug_features'],\n",
    "                         feature_subsample=gParameters['feature_subsample'],\n",
    "                         scaling=gParameters['scaling'],\n",
    "                         scramble=gParameters['scramble'],\n",
    "                         min_logconc=gParameters['min_logconc'],\n",
    "                         max_logconc=gParameters['max_logconc'],\n",
    "                         subsample=gParameters['subsample'],\n",
    "                         category_cutoffs=gParameters['category_cutoffs'])\n",
    "\n",
    "# Initialize weights and learning rule\n",
    "initializer_weights = candle.build_initializer(gParameters['initialization'], kerasDefaults, seed)\n",
    "initializer_bias = candle.build_initializer('constant', kerasDefaults, 0.)\n",
    "\n",
    "activation = gParameters['activation']\n",
    "\n",
    "# Define model architecture\n",
    "gen_shape = None\n",
    "out_dim = 1\n",
    "\n",
    "model = Sequential()\n",
    "if 'dense' in gParameters: # Build dense layers\n",
    "    for layer in gParameters['dense']:\n",
    "        if layer:\n",
    "            model.add(Dense(layer, input_dim=loader.input_dim,\n",
    "                        kernel_initializer=initializer_weights,\n",
    "                        bias_initializer=initializer_bias))\n",
    "            if gParameters['batch_normalization']:\n",
    "                model.add(BatchNormalization())\n",
    "            model.add(Activation(gParameters['activation']))\n",
    "            if gParameters['dropout']:\n",
    "                model.add(Dropout(gParameters['dropout']))\n",
    "else: # Build convolutional layers\n",
    "    gen_shape = 'add_1d'\n",
    "    layer_list = list(range(0, len(gParameters['conv'])))\n",
    "    lc_flag=False\n",
    "    if 'locally_connected' in gParameters:\n",
    "        lc_flag = True\n",
    "\n",
    "    for l, i in enumerate(layer_list):\n",
    "        if i == 0:\n",
    "            add_conv_layer(model, gParameters['conv'][i], input_dim=loader.input_dim,locally_connected=lc_flag)\n",
    "        else:\n",
    "            add_conv_layer(model, gParameters['conv'][i],locally_connected=lc_flag)\n",
    "        if gParameters['batch_normalization']:\n",
    "                model.add(BatchNormalization())\n",
    "        model.add(Activation(gParameters['activation']))\n",
    "        if gParameters['pool']:\n",
    "            model.add(MaxPooling1D(pool_size=gParameters['pool']))\n",
    "    model.add(Flatten())\n",
    "\n",
    "model.add(Dense(out_dim))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = candle.build_optimizer(gParameters['optimizer'],\n",
    "                                            gParameters['learning_rate'],\n",
    "                                            kerasDefaults)\n",
    "\n",
    "from keras.models import model_from_json, load_model\n",
    "# load json and create model\n",
    "trained_model_json = 'p1b3.model.json'\n",
    "json_data_url = 'https://modac.cancer.gov/api/v2/dataObject/NCI_DOE_Archive/JDACS4C/JDACS4C_Pilot_1/single_drug_response_predictor_p1b3/' \\\n",
    "                + trained_model_json\n",
    "candle.get_file(trained_model_json, json_data_url, datadir=\".\")\n",
    "json_file = open(trained_model_json, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model_json = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "trained_model_h5 = 'p1b3.model.h5'\n",
    "h5_data_url = 'https://modac.cancer.gov/api/v2/dataObject/NCI_DOE_Archive/JDACS4C/JDACS4C_Pilot_1/single_drug_response_predictor_p1b3/' \\\n",
    "              + trained_model_h5\n",
    "candle.get_file(trained_model_h5, h5_data_url, datadir=\".\")\n",
    "loaded_model_json.load_weights(trained_model_h5)\n",
    "print(\"Loaded model weights from disk\")\n",
    "\n",
    "loaded_model_json.compile(loss=gParameters['loss'], optimizer=optimizer)\n",
    "print(loaded_model_json.summary())\n",
    "print(\"Loaded model from disk\")\n",
    "model = loaded_model_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64d50be7-4ff7-4c84-9fbb-6b5f8d5fc374",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = benchmark.DataGenerator(loader, partition='test', batch_size=gParameters['batch_size'], shape=gen_shape, name='test_gen').flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ae2688e-6a2e-46da-8b9a-e4cba3b75388",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps = int(loader.n_test/gParameters['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62a955e0-9625-4be8-8399-af65654124e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test_steps' in gParameters:\n",
    "    test_steps = gParameters['test_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e34a0622-515c-4eb8-83b8-c6f34388004d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (40037,) but got array with shape (29532,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5caac321834d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_loss, test_acc, _, _, _, _ = evaluate_model(model, test_gen, test_steps, metric=gParameters['loss'],\n\u001b[0;32m----> 2\u001b[0;31m                                                      category_cutoffs=gParameters['category_cutoffs'])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-d57dc1879f69>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, generator, steps, metric, category_cutoffs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0my_batch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0my_batch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/homes/v/vineethg/.conda/envs/Combo/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1572\u001b[0m             \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mof\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m         \"\"\"\n\u001b[0;32m-> 1574\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/homes/v/vineethg/.conda/envs/Combo/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/homes/v/vineethg/.conda/envs/Combo/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (40037,) but got array with shape (29532,)"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, _, _, _, _ = evaluate_model(model, test_gen, test_steps, metric=gParameters['loss'],\n",
    "                                                     category_cutoffs=gParameters['category_cutoffs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b80eb41-da48-4e03-9e38-07f11c39e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0f39ea6-adb3-4e78-b5b8-e0ba2be6a68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 29532)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f16219e-1812-413d-a3e9-2f3f3f5e0f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6567c8-7130-45b0-8749-c026e72f9b59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combo",
   "language": "python",
   "name": "combo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
